{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "import os, shutil\n",
    "from glob import glob\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-e3017a5bfa19>:17: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  xy = np.asarray(bounds.centroid[0].xy).squeeze()\n"
     ]
    }
   ],
   "source": [
    "# reads my boundary shapefile \n",
    "bounds = gpd.read_file('../GeoData/boundaryFinal.gpkg')\n",
    "\n",
    "# creates path variables for WRS and landsat folders \n",
    "WRS_PATH = '../GeoData/Landsat8/WRS2_descending_0.zip'\n",
    "LANDSAT_PATH = os.path.dirname(WRS_PATH)\n",
    "\n",
    "shutil.unpack_archive(WRS_PATH, os.path.join(LANDSAT_PATH, 'wrs2'))\n",
    "\n",
    "wrs = gpd.GeoDataFrame.from_file('../GeoData/Landsat8/wrs2/WRS2_descending.shp')\n",
    "wrs_intersection = wrs[wrs.intersects(bounds.geometry[0])]\n",
    "\n",
    "# creates path (unrelated to file architecture paths) and row variables for wrs2-boundary intersection\n",
    "paths, rows = wrs_intersection['PATH'].values, wrs_intersection['ROW'].values\n",
    "\n",
    "# get the center of the map\n",
    "xy = np.asarray(bounds.centroid[0].xy).squeeze()\n",
    "center = list(xy[::-1])\n",
    "\n",
    "# select a zoom\n",
    "zoom = 6\n",
    "\n",
    "# create the most basic OSM folium map\n",
    "m = folium.Map(location=center, zoom_start=zoom, control_scale=True)\n",
    "\n",
    "# add the bounds GeoDataFrame\n",
    "m.add_child(folium.GeoJson(bounds.__geo_interface__, name='Maricopa County', \n",
    "                           style_function=lambda x: {'color': 'red', 'alpha': 0}))\n",
    "\n",
    "# iterate through each polygon of paths and rows intersecting the area\n",
    "for i, row in wrs_intersection.iterrows():\n",
    "    # create a string for the name containing the path and row of this polygon\n",
    "    name = 'path: %03d, row: %03d' % (row.PATH, row.ROW)\n",
    "    # create the folium geometry of this polygon \n",
    "    g = folium.GeoJson(row.geometry.__geo_interface__, name=name)\n",
    "    # add a folium popup object with the name string\n",
    "    g.add_child(folium.Popup(name))\n",
    "    # add the object to the map\n",
    "    g.add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "m.save('../GeoData/images/wrs.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: 37 Row: 36\n",
      " Found 1 images\n",
      "\n",
      "Path: 36 Row: 36\n",
      " Found 1 images\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # finds scenes that match criteria and creates list of selection\n",
    "\n",
    "# scene_selection = pd.read_csv('../GeoData/Landsat8/index.csv', chunksize=1000)\n",
    "\n",
    "# bulk_list = []\n",
    "\n",
    "# # iterate through paths and rows\n",
    "\n",
    "# for chunk in scene_selection: \n",
    "    \n",
    "#     for path, row in zip(paths, rows):\n",
    "\n",
    "#         # filter the google table for images matching path, row, satellite, datetime, cloudcover, processing state\n",
    "#         scenes = chunk[(chunk.WRS_PATH == path) & (chunk.WRS_ROW == row) & \n",
    "#                            (chunk.CLOUD_COVER <= 5) & \n",
    "#                            (~chunk.PRODUCT_ID.str.contains('_T2')) &\n",
    "#                            (~chunk.PRODUCT_ID.str.contains('_RT')) &\n",
    "#                            (chunk.SPACECRAFT_ID == 'LANDSAT_8') & \n",
    "#                            (chunk.SENSING_TIME > '2018-01-01')]\n",
    "        \n",
    "#         if len(scenes) > 0: \n",
    "#             print('Path:',path, 'Row:', row)\n",
    "#             print(' Found {} images\\n'.format(len(scenes)))\n",
    "\n",
    "#         if len(scenes):\n",
    "#             scenes = scenes.sort_values('CLOUD_COVER').iloc[0]\n",
    "\n",
    "#         if len(scenes) > 0:\n",
    "#             bulk_list.append(scenes)\n",
    "            \n",
    "# print(len(bulk_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0               SCENE_ID  \\\n",
      "0     6502515  LC80370382020269LGN00   \n",
      "\n",
      "                                 PRODUCT_ID SPACECRAFT_ID SENSOR_ID  \\\n",
      "0  LC08_L1TP_037038_20200925_20201006_01_T1     LANDSAT_8  OLI_TIRS   \n",
      "\n",
      "  DATE_ACQUIRED  COLLECTION_NUMBER COLLECTION_CATEGORY  \\\n",
      "0    2020-09-25                  1                  T1   \n",
      "\n",
      "                   SENSING_TIME DATA_TYPE  WRS_PATH  WRS_ROW  CLOUD_COVER  \\\n",
      "0  2020-09-25T18:04:42.2461660Z      L1TP        37       38          0.0   \n",
      "\n",
      "   NORTH_LAT  SOUTH_LAT   WEST_LON   EAST_LON  TOTAL_SIZE  \\\n",
      "0   32.81957   30.64602 -114.18259 -111.68175  1003553577   \n",
      "\n",
      "                                            BASE_URL  \n",
      "0  gs://gcp-public-data-landsat/LC08/01/037/038/L...  \n",
      "\n",
      " EntityId: LC08_L1TP_037038_20200925_20201006_01_T1 \n",
      "\n",
      " Checking content:  \n",
      "\n",
      "  Downloading: LC08_L1TP_037038_20200925_20201006_01_T1_BQA.TIF\n",
      "  Downloading: LC08_L1TP_037038_20200925_20201006_01_T1_B2.TIF\n",
      "  Downloading: LC08_L1TP_037038_20200925_20201006_01_T1_ANG.txt\n",
      "  Downloading: LC08_L1TP_037038_20200925_20201006_01_T1_B3.TIF\n",
      "  Downloading: LC08_L1TP_037038_20200925_20201006_01_T1_B2.TIF.ovr\n",
      "  Downloading: LC08_L1TP_037038_20200925_20201006_01_T1_B5.TIF\n",
      "  Downloading: LC08_L1TP_037038_20200925_20201006_01_T1_B11.TIF\n",
      "  Downloading: LC08_L1TP_037038_20200925_20201006_01_T1_B10.TIF.ovr\n",
      "  Downloading: LC08_L1TP_037038_20200925_20201006_01_T1_B9.TIF\n",
      "  Downloading: LC08_L1TP_037038_20200925_20201006_01_T1_BQA.TIF.ovr\n",
      "  Downloading: LC08_L1TP_037038_20200925_20201006_01_T1_MTL.txt\n",
      "  Downloading: LC08_L1TP_037038_20200925_20201006_01_T1_B11.TIF.ovr\n",
      "  Downloading: LC08_L1TP_037038_20200925_20201006_01_T1_B11_wrk.IMD\n",
      "  Downloading: LC08_L1TP_037038_20200925_20201006_01_T1_B6_wrk.IMD\n",
      "  Downloading: LC08_L1TP_037038_20200925_20201006_01_T1_B2_wrk.IMD\n",
      "  Downloading: LC08_L1TP_037038_20200925_20201006_01_T1_B7.TIF.ovr\n",
      "  Downloading: LC08_L1TP_037038_20200925_20201006_01_T1_B3.TIF.ovr\n",
      "  Downloading: LC08_L1TP_037038_20200925_20201006_01_T1_B10_wrk.IMD\n",
      "  Downloading: LC08_L1TP_037038_20200925_20201006_01_T1_B5_wrk.IMD\n",
      "  Downloading: LC08_L1TP_037038_20200925_20201006_01_T1_B8.TIF\n"
     ]
    }
   ],
   "source": [
    "# creates dataframe object and writes csv file of scene selection\n",
    "\n",
    "# bulk_frame = pd.concat(bulk_list, 1).T\n",
    "# bulk_frame = bulk_frame.sort_values(by=['DATE_ACQUIRED'], ascending=False)\n",
    "bulk_frame = pd.read_csv('scene_selection.csv')\n",
    "\n",
    "test_frame = bulk_frame[bulk_frame['PRODUCT_ID'] == 'LC08_L1TP_037038_20200925_20201006_01_T1']\n",
    "\n",
    "print(test_frame)\n",
    "\n",
    "# uses scene selection to download scenes from amazon server\n",
    "\n",
    "for i, row in test_frame.iterrows():\n",
    "    \n",
    "    print('\\n', 'EntityId:', row.PRODUCT_ID, '\\n')\n",
    "    print(' Checking content: ', '\\n')\n",
    "    \n",
    "    test_url = 'https://landsat-pds.s3.amazonaws.com/c1/L8/37/38/LC08_L1TP_037038_20200925_20201006_01_T1/index.html'\n",
    "    \n",
    "    row_url = 'https://landsat-pds.s3.amazonaws.com/c1/L8/0{}/0{}/{}/index.html'.format(row.WRS_PATH, row.WRS_ROW, row.PRODUCT_ID)\n",
    "    \n",
    "    response = requests.get(row_url)\n",
    "    \n",
    "#     if response.status_code == 200:\n",
    "\n",
    "    # Import the html to beautiful soup\n",
    "    html = BeautifulSoup(response.content, 'html.parser')\n",
    "    entity_dir = os.path.join(LANDSAT_PATH, row.PRODUCT_ID)\n",
    "    os.makedirs(entity_dir, exist_ok=True)\n",
    "\n",
    "    for li in html.find_all('li'):\n",
    "\n",
    "        # Get the href tag\n",
    "        file = li.find_next('a').get('href')\n",
    "\n",
    "        print('  Downloading: {}'.format(file))\n",
    "\n",
    "        # Download the files\n",
    "        # code from: https://stackoverflow.com/a/18043472/5361345\n",
    "\n",
    "        response = requests.get(row_url.replace('index.html', file), stream=True)\n",
    "\n",
    "        with open(os.path.join(entity_dir, file), 'wb') as output:\n",
    "            shutil.copyfileobj(response.raw, output)\n",
    "        del response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
